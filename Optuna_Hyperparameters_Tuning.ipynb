{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF9_LiGqMp08"
      },
      "source": [
        "# **Optuna Objective Function - Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnZTBwYfL_QV"
      },
      "outputs": [],
      "source": [
        "def objective_classification(trial):\n",
        "    model_name = trial.suggest_categorical(\n",
        "        \"model\",\n",
        "        [\n",
        "            \"Logistic Regression\", \"Random Forest\", \"Xgboost\", \"SVM\", \"GradientBoosting\",\n",
        "            \"LightGBM\", \"CatBoost\", \"KNN\",\n",
        "            \"Linear\", \"Lasso\", \"Ridge\", \"ElasticNet\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Logistic Regression ---> max_iter = 2000 - 5000\n",
        "    if model_name == \"Logistic Regression\":\n",
        "        c = trial.suggest_float(\"lr_C\", 0.1, 100, log=True)\n",
        "        solver = trial.suggest_categorical(\"lr_solver\", [\"liblinear\", \"saga\", \"lbfgs\", \"newton-cg\"])\n",
        "        penalty = None\n",
        "        if solver in [\"lbfgs\", \"newton-cg\"]:\n",
        "            penalty = trial.suggest_categorical(\"lr_penalty_lbfgs_nc\", [\"l2\", None])\n",
        "        elif solver == \"liblinear\":\n",
        "            penalty = trial.suggest_categorical(\"lr_penalty_liblinear\", [\"l1\", \"l2\"])\n",
        "        else:  # saga\n",
        "            penalty = trial.suggest_categorical(\"lr_penalty_saga\", [\"l1\", \"l2\", \"elasticnet\", None])\n",
        "        l1_ratio = trial.suggest_float(\"lr_l1_ratio\", 0.0, 1.0) if penalty == \"elasticnet\" else None\n",
        "    #max_iter = trial.suggest_int(\"lr_max_iter\", 100, 5000, step=100)\n",
        "        model = LogisticRegression(\n",
        "            C=c, solver=solver, penalty=penalty,\n",
        "            l1_ratio=l1_ratio, max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    # Random Forest\n",
        "    elif model_name == \"Random Forest\":\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=trial.suggest_int(\"rf_n_estimators\", 50, 300),\n",
        "            max_depth=trial.suggest_int(\"rf_max_depth\", 3, 20),\n",
        "            min_samples_split=trial.suggest_int(\"rf_min_samples_split\", 2, 10),\n",
        "            min_samples_leaf=trial.suggest_int(\"rf_min_samples_leaf\", 1, 10),\n",
        "            bootstrap=trial.suggest_categorical(\"rf_bootstrap\", [True, False]),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    # Decision Tree\n",
        "    if model_name == \"DecisionTreeClassifier\":\n",
        "        # Classification hyperparameters\n",
        "        max_depth = trial.suggest_int(\"dtc_max_depth\", 2, 20)\n",
        "        criterion = trial.suggest_categorical(\"dtc_criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
        "        model = DecisionTreeClassifier(\n",
        "            max_depth=max_depth,\n",
        "            criterion=criterion,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    # XGBoost\n",
        "    elif model_name == \"Xgboost\":\n",
        "        model = XGBClassifier(\n",
        "            learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"xgb_max_depth\", 2, 12),\n",
        "            n_estimators=trial.suggest_int(\"xgb_n_estimators\", 50, 500),\n",
        "            subsample=trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
        "            colsample_bytree=trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0),\n",
        "            gamma=trial.suggest_float(\"xgb_gamma\", 0, 5),\n",
        "            min_child_weight=trial.suggest_int(\"xgb_min_child_weight\", 1, 10),\n",
        "            reg_alpha=trial.suggest_float(\"xgb_reg_alpha\", 0.0, 5.0),\n",
        "            reg_lambda=trial.suggest_float(\"xgb_reg_lambda\", 0.0, 5.0),\n",
        "            random_state=42, use_label_encoder=False, eval_metric=\"logloss\"\n",
        "        )\n",
        "\n",
        "    # SVM\n",
        "    elif model_name == \"SVM\":\n",
        "        model = SVC(\n",
        "            C=trial.suggest_float(\"svm_C\", 0.1, 100, log=True),\n",
        "            kernel=trial.suggest_categorical(\"svm_kernel\", [\"linear\", \"rbf\", \"poly\", \"sigmoid\"]),\n",
        "            gamma=trial.suggest_categorical(\"svm_gamma\", [\"scale\", \"auto\"]),\n",
        "            probability=True, random_state=42\n",
        "        )\n",
        "\n",
        "    # Gradient Boosting\n",
        "    elif model_name == \"GradientBoosting\":\n",
        "        model = GradientBoostingClassifier(\n",
        "            n_estimators=trial.suggest_int(\"gb_n_estimators\", 50, 500),\n",
        "            learning_rate=trial.suggest_float(\"gb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"gb_max_depth\", 2, 10),\n",
        "            subsample=trial.suggest_float(\"gb_subsample\", 0.5, 1.0),\n",
        "            min_samples_split=trial.suggest_int(\"gb_min_samples_split\", 2, 20),\n",
        "            min_samples_leaf=trial.suggest_int(\"gb_min_samples_leaf\", 1, 20),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    # LightGBM\n",
        "    elif model_name == \"LightGBM\":\n",
        "        model = LGBMClassifier(\n",
        "            n_estimators=trial.suggest_int(\"lgb_n_estimators\", 50, 500),\n",
        "            learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"lgb_max_depth\", -1, 20),\n",
        "            num_leaves=trial.suggest_int(\"lgb_num_leaves\", 20, 300),\n",
        "            subsample=trial.suggest_float(\"lgb_subsample\", 0.5, 1.0),\n",
        "            colsample_bytree=trial.suggest_float(\"lgb_colsample_bytree\", 0.5, 1.0),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    # CatBoost\n",
        "    elif model_name == \"CatBoost\":\n",
        "        model = CatBoostClassifier(\n",
        "            iterations=trial.suggest_int(\"cat_iterations\", 100, 500),\n",
        "            depth=trial.suggest_int(\"cat_depth\", 3, 10),\n",
        "            learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3, log=True),\n",
        "            l2_leaf_reg=trial.suggest_float(\"cat_l2_leaf_reg\", 1.0, 10.0),\n",
        "            verbose=0, random_state=42\n",
        "        )\n",
        "\n",
        "    # KNN\n",
        "    elif model_name == \"KNN\":\n",
        "        model = KNeighborsClassifier(\n",
        "            n_neighbors=trial.suggest_int(\"knn_n_neighbors\", 3, 30),\n",
        "            weights=trial.suggest_categorical(\"knn_weights\", [\"uniform\", \"distance\"]),\n",
        "            p=trial.suggest_int(\"knn_p\", 1, 2)\n",
        "        )\n",
        "\n",
        "    # Linear / Lasso / Ridge / ElasticNet\n",
        "    elif model_name == \"Linear\":\n",
        "        model = LogisticRegression(max_iter=5000, random_state=42)\n",
        "\n",
        "    elif model_name == \"Lasso\":\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"l1\", solver=\"saga\",\n",
        "            C=trial.suggest_float(\"lasso_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"Ridge\":\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"l2\", solver=\"saga\",\n",
        "            C=trial.suggest_float(\"ridge_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    else:  # ElasticNet\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"elasticnet\", solver=\"saga\",\n",
        "            l1_ratio=trial.suggest_float(\"elastic_l1_ratio\", 0.0, 1.0),\n",
        "            C=trial.suggest_float(\"elastic_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "   # Linear Regression (no penalty â†’ only certain solvers allowed)\n",
        "    elif model_name == \"Linear\":\n",
        "        solver = trial.suggest_categorical(\"linear_solver\", [\"lbfgs\", \"newton-cg\", \"sag\", \"saga\"])\n",
        "        model = LogisticRegression(\n",
        "            penalty=None, solver=solver,\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    # Lasso (L1)\n",
        "    elif model_name == \"Lasso\":\n",
        "        solver = trial.suggest_categorical(\"lasso_solver\", [\"liblinear\", \"saga\"])\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"l1\", solver=solver,\n",
        "            C=trial.suggest_float(\"lasso_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    # Ridge (L2)\n",
        "    elif model_name == \"Ridge\":\n",
        "        solver = trial.suggest_categorical(\"ridge_solver\", [\"lbfgs\", \"newton-cg\", \"sag\", \"saga\", \"liblinear\"])\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"l2\", solver=solver,\n",
        "            C=trial.suggest_float(\"ridge_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    # ElasticNet (only saga)\n",
        "    elif model_name == \"ElasticNet\":\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"elasticnet\", solver=\"saga\",\n",
        "            l1_ratio=trial.suggest_float(\"elastic_l1_ratio\", 0.0, 1.0),\n",
        "            C=trial.suggest_float(\"elastic_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "    \n",
        "        # ---------------- Pipeline ----------------\n",
        "    pipeline = Pipeline([\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"classifier\", model)\n",
        "    ])\n",
        "\n",
        "    # Use n_jobs=1 to avoid Windows parallelism errors\n",
        "    scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=1)\n",
        "    return scores.mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqxIv0DSMxKL"
      },
      "source": [
        "# **Optuna Objective Function - Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tabj2cdRMmLt"
      },
      "outputs": [],
      "source": [
        "def objective_regression(trial):\n",
        "    model_name = trial.suggest_categorical(\n",
        "        \"model\",\n",
        "        [\n",
        "            \"Random Forest\", \"Xgboost\", \"SVM\", \"GradientBoosting\",\n",
        "            \"LightGBM\", \"CatBoost\", \"KNN\",\n",
        "            \"Linear\", \"Lasso\", \"Ridge\", \"ElasticNet\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if model_name == \"Random Forest\":\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=trial.suggest_int(\"rf_n_estimators\", 50, 300),\n",
        "            max_depth=trial.suggest_int(\"rf_max_depth\", 3, 20),\n",
        "            min_samples_split=trial.suggest_int(\"rf_min_samples_split\", 2, 10),\n",
        "            min_samples_leaf=trial.suggest_int(\"rf_min_samples_leaf\", 1, 10),\n",
        "            bootstrap=trial.suggest_categorical(\"rf_bootstrap\", [True, False]),\n",
        "            random_state=42\n",
        "        )\n",
        "     elif model_name == \"DecisionTreeRegressor\":\n",
        "        # Regression hyperparameters\n",
        "        max_depth = trial.suggest_int(\"dtr_max_depth\", 2, 20)\n",
        "        criterion = trial.suggest_categorical(\"dtr_criterion\", [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"])\n",
        "        model = DecisionTreeRegressor(\n",
        "            max_depth=max_depth,\n",
        "            criterion=criterion,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"Xgboost\":\n",
        "        model = XGBRegressor(\n",
        "            learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"xgb_max_depth\", 2, 12),\n",
        "            n_estimators=trial.suggest_int(\"xgb_n_estimators\", 50, 500),\n",
        "            subsample=trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
        "            colsample_bytree=trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0),\n",
        "            gamma=trial.suggest_float(\"xgb_gamma\", 0, 5),\n",
        "            reg_alpha=trial.suggest_float(\"xgb_reg_alpha\", 0.0, 5.0),\n",
        "            reg_lambda=trial.suggest_float(\"xgb_reg_lambda\", 0.0, 5.0),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"SVM\":\n",
        "        model = SVR(\n",
        "            C=trial.suggest_float(\"svm_C\", 0.1, 100, log=True),\n",
        "            kernel=trial.suggest_categorical(\"svm_kernel\", [\"linear\", \"rbf\", \"poly\", \"sigmoid\"]),\n",
        "            gamma=trial.suggest_categorical(\"svm_gamma\", [\"scale\", \"auto\"])\n",
        "        )\n",
        "\n",
        "    elif model_name == \"GradientBoosting\":\n",
        "        model = GradientBoostingRegressor(\n",
        "            n_estimators=trial.suggest_int(\"gb_n_estimators\", 50, 500),\n",
        "            learning_rate=trial.suggest_float(\"gb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"gb_max_depth\", 2, 10),\n",
        "            subsample=trial.suggest_float(\"gb_subsample\", 0.5, 1.0),\n",
        "            min_samples_split=trial.suggest_int(\"gb_min_samples_split\", 2, 20),\n",
        "            min_samples_leaf=trial.suggest_int(\"gb_min_samples_leaf\", 1, 20),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"LightGBM\":\n",
        "        model = LGBMRegressor(\n",
        "            n_estimators=trial.suggest_int(\"lgb_n_estimators\", 50, 500),\n",
        "            learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"lgb_max_depth\", -1, 20),\n",
        "            num_leaves=trial.suggest_int(\"lgb_num_leaves\", 20, 300),\n",
        "            subsample=trial.suggest_float(\"lgb_subsample\", 0.5, 1.0),\n",
        "            colsample_bytree=trial.suggest_float(\"lgb_colsample_bytree\", 0.5, 1.0),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"CatBoost\":\n",
        "        model = CatBoostRegressor(\n",
        "            iterations=trial.suggest_int(\"cat_iterations\", 100, 500),\n",
        "            depth=trial.suggest_int(\"cat_depth\", 3, 10),\n",
        "            learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3, log=True),\n",
        "            l2_leaf_reg=trial.suggest_float(\"cat_l2_leaf_reg\", 1.0, 10.0),\n",
        "            verbose=0, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"KNN\":\n",
        "        model = KNeighborsRegressor(\n",
        "            n_neighbors=trial.suggest_int(\"knn_n_neighbors\", 3, 30),\n",
        "            weights=trial.suggest_categorical(\"knn_weights\", [\"uniform\", \"distance\"]),\n",
        "            p=trial.suggest_int(\"knn_p\", 1, 2)\n",
        "        )\n",
        "\n",
        "    elif model_name == \"Linear\":\n",
        "        model = LinearRegression()\n",
        "\n",
        "    elif model_name == \"Lasso\":\n",
        "        model = Lasso(\n",
        "            alpha=trial.suggest_float(\"lasso_alpha\", 0.0001, 1.0, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"Ridge\":\n",
        "        model = Ridge(\n",
        "            alpha=trial.suggest_float(\"ridge_alpha\", 0.0001, 10.0, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    else:  # ElasticNet\n",
        "        model = ElasticNet(\n",
        "            alpha=trial.suggest_float(\"elastic_alpha\", 0.0001, 1.0, log=True),\n",
        "            l1_ratio=trial.suggest_float(\"elastic_l1_ratio\", 0.0, 1.0),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    \n",
        "    # ---------------- Pipeline ----------------\n",
        "    pipeline = Pipeline([\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"classifier\", model)\n",
        "    ])\n",
        "\n",
        "\n",
        "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"r2\").mean()\n",
        "    return score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 6. Run Optuna Study\n",
        "# ===============================\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=30)  # Increase trials for better results\n",
        "\n",
        "print(\"Best Params:\", study.best_params)\n",
        "best_model_name = study.best_params[\"model\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_params_rf = study.best_params.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OR,\n",
        "\n",
        "best_model_name = best_params_rf.pop(\"model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”‘ Rule of Thumb:\n",
        "\n",
        "- If you tune multiple models in one objective â†’ use .pop(\"model\").\n",
        "\n",
        "- If you tune only one model â†’ donâ€™t use .pop(\"model\"), just assign the model name manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 7. Train Best Model on Full Training Data\n",
        "# ===============================\n",
        "# Recreate best model with params (you can extract these from study.best_params)\n",
        "# Here we only demo with RandomForest as example (adapt as per best_model_name)\n",
        "\n",
        "best_model = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", RandomForestClassifier(\n",
        "        n_estimators=best_params_rf.get(\"rf_n_estimators\", 100),\n",
        "        max_depth=best_params_rf.get(\"rf_max_depth\", None),\n",
        "        min_samples_split=best_params_rf.get(\"rf_min_samples_split\", 2),\n",
        "        min_samples_leaf=best_params_rf.get(\"rf_min_samples_leaf\", 1),\n",
        "        bootstrap=best_params_rf.get(\"rf_bootstrap\", True),\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Best Hyperparameters Choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "def objective_classification(trial):\n",
        "    model_name = trial.suggest_categorical(\"model\", [\n",
        "        \"LogisticRegression\", \"Ridge\", \"RandomForest\", \"DecisionTree\",\n",
        "        \"GradientBoosting\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"KNN\"\n",
        "    ])\n",
        "\n",
        "    if model_name == \"LogisticRegression\":\n",
        "        # --- Logistic Regression special handling ---\n",
        "        C = trial.suggest_float(\"lr_C\", 0.001, 10, log=True)\n",
        "        solver = trial.suggest_categorical(\"lr_solver\", [\"liblinear\", \"saga\", \"lbfgs\", \"newton-cg\"])\n",
        "\n",
        "        penalty = None\n",
        "        if solver in [\"lbfgs\", \"newton-cg\"]:\n",
        "            penalty = trial.suggest_categorical(\"lr_penalty_lbfgs_nc\", [\"l2\", None])\n",
        "        elif solver == \"liblinear\":\n",
        "            penalty = trial.suggest_categorical(\"lr_penalty_liblinear\", [\"l1\", \"l2\"])\n",
        "        else:  # saga\n",
        "            penalty = trial.suggest_categorical(\"lr_penalty_saga\", [\"l1\", \"l2\", \"elasticnet\", None])\n",
        "\n",
        "        l1_ratio = trial.suggest_float(\"lr_l1_ratio\", 0.0, 1.0) if penalty == \"elasticnet\" else None\n",
        "\n",
        "        model = LogisticRegression(\n",
        "            C=C, solver=solver, penalty=penalty,\n",
        "            l1_ratio=l1_ratio, max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"Ridge\":\n",
        "        alpha = trial.suggest_float(\"alpha\", 0.001, 10, log=True)\n",
        "        model = RidgeClassifier(alpha=alpha, random_state=42)\n",
        "\n",
        "    elif model_name == \"RandomForest\":\n",
        "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 300, step=50)\n",
        "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=n_estimators, max_depth=max_depth,\n",
        "            n_jobs=-1, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"DecisionTree\":\n",
        "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
        "        criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
        "        model = DecisionTreeClassifier(max_depth=max_depth, criterion=criterion, random_state=42)\n",
        "\n",
        "    elif model_name == \"GradientBoosting\":\n",
        "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 300, step=50)\n",
        "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
        "        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "        model = GradientBoostingClassifier(\n",
        "            n_estimators=n_estimators, learning_rate=learning_rate,\n",
        "            max_depth=max_depth, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"XGBoost\":\n",
        "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 300, step=50)\n",
        "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
        "        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=n_estimators, learning_rate=learning_rate,\n",
        "            max_depth=max_depth, eval_metric=\"logloss\",\n",
        "            random_state=42, n_jobs=-1, use_label_encoder=False\n",
        "        )\n",
        "\n",
        "    elif model_name == \"LightGBM\":\n",
        "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 300, step=50)\n",
        "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
        "        num_leaves = trial.suggest_int(\"num_leaves\", 20, 80)\n",
        "        model = LGBMClassifier(\n",
        "            n_estimators=n_estimators, learning_rate=learning_rate,\n",
        "            num_leaves=num_leaves, random_state=42, n_jobs=-1\n",
        "        )\n",
        "\n",
        "    elif model_name == \"CatBoost\":\n",
        "        depth = trial.suggest_int(\"depth\", 4, 10)\n",
        "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
        "        iterations = trial.suggest_int(\"iterations\", 100, 300, step=50)\n",
        "        model = CatBoostClassifier(\n",
        "            depth=depth, learning_rate=learning_rate,\n",
        "            iterations=iterations, verbose=0, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"KNN\":\n",
        "        n_neighbors = trial.suggest_int(\"n_neighbors\", 3, 25)\n",
        "        weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
        "        model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
        "\n",
        "    # cross-validation with accuracy\n",
        "    score = cross_val_score(model, X, y, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
        "    return np.mean(score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "def objective_regression(trial):\n",
        "    model_name = trial.suggest_categorical(\"model\", [\n",
        "        \"LinearRegression\", \"Ridge\", \"Lasso\", \"ElasticNet\",\n",
        "        \"RandomForest\", \"DecisionTree\", \"GradientBoosting\",\n",
        "        \"XGBoost\", \"LightGBM\", \"CatBoost\", \"KNN\"\n",
        "    ])\n",
        "\n",
        "    if model_name == \"LinearRegression\":\n",
        "        model = LinearRegression()\n",
        "\n",
        "    elif model_name == \"Ridge\":\n",
        "        alpha = trial.suggest_float(\"alpha\", 0.001, 10, log=True)\n",
        "        model = Ridge(alpha=alpha, random_state=42)\n",
        "\n",
        "    elif model_name == \"Lasso\":\n",
        "        alpha = trial.suggest_float(\"alpha\", 0.001, 10, log=True)\n",
        "        model = Lasso(alpha=alpha, max_iter=3000, random_state=42)\n",
        "\n",
        "    elif model_name == \"ElasticNet\":\n",
        "        alpha = trial.suggest_float(\"alpha\", 0.001, 10, log=True)\n",
        "        l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0)\n",
        "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=3000, random_state=42)\n",
        "\n",
        "    elif model_name == \"RandomForest\":\n",
        "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 300, step=50)\n",
        "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=n_estimators, max_depth=max_depth,\n",
        "            n_jobs=-1, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"DecisionTree\":\n",
        "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
        "        criterion = trial.suggest_categorical(\"criterion\", [\"squared_error\", \"friedman_mse\"])\n",
        "        model = DecisionTreeRegressor(max_depth=max_depth, criterion=criterion, random_state=42)\n",
        "\n",
        "    elif model_name == \"GradientBoosting\":\n",
        "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 300, step=50)\n",
        "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
        "        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "        model = GradientBoostingRegressor(\n",
        "            n_estimators=n_estimators, learning_rate=learning_rate,\n",
        "            max_depth=max_depth, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"XGBoost\":\n",
        "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 300, step=50)\n",
        "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
        "        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "        model = XGBRegressor(\n",
        "            n_estimators=n_estimators, learning_rate=learning_rate,\n",
        "            max_depth=max_depth, random_state=42, n_jobs=-1\n",
        "        )\n",
        "\n",
        "    elif model_name == \"LightGBM\":\n",
        "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 300, step=50)\n",
        "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
        "        num_leaves = trial.suggest_int(\"num_leaves\", 20, 80)\n",
        "        model = LGBMRegressor(\n",
        "            n_estimators=n_estimators, learning_rate=learning_rate,\n",
        "            num_leaves=num_leaves, random_state=42, n_jobs=-1\n",
        "        )\n",
        "\n",
        "    elif model_name == \"CatBoost\":\n",
        "        depth = trial.suggest_int(\"depth\", 4, 10)\n",
        "        learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3)\n",
        "        iterations = trial.suggest_int(\"iterations\", 100, 300, step=50)\n",
        "        model = CatBoostRegressor(\n",
        "            depth=depth, learning_rate=learning_rate,\n",
        "            iterations=iterations, verbose=0, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"KNN\":\n",
        "        n_neighbors = trial.suggest_int(\"n_neighbors\", 3, 25)\n",
        "        weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
        "        model = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights)\n",
        "\n",
        "    # cross-validation with r2 score\n",
        "    score = cross_val_score(model, X, y, cv=3, scoring=\"r2\", n_jobs=-1)\n",
        "    return np.mean(score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using optuna to find the best parameters for LightGBM\n",
        "\n",
        "def lgb_objective(trial):\n",
        "    lgb_params = {\n",
        "    'learning_rate' : trial.suggest_float('learning_rate' , 0.01 , 0.1 , log=True),\n",
        "    'num_leaves' : trial.suggest_int('num_leaves' , 5 , 50),\n",
        "    'max_depth' : trial.suggest_int('max_depth' , 3 , 15),\n",
        "    'reg_alpha' : trial.suggest_float('reg_alpha' , 0.01 , 0.1 , log=True),\n",
        "    'reg_lambda' : trial.suggest_float('reg_lambda' , 0.01 , 0.1 , log=True),\n",
        "    'subsample' : trial.suggest_float('subsample' , 0 , 1)\n",
        "    }"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
