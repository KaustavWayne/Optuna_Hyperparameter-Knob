{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Optuna Objective Function - Classification**"
      ],
      "metadata": {
        "id": "hF9_LiGqMp08"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnZTBwYfL_QV"
      },
      "outputs": [],
      "source": [
        "def objective_classification(trial):\n",
        "    model_name = trial.suggest_categorical(\n",
        "        \"model\",\n",
        "        [\n",
        "            \"Logistic Regression\", \"Random Forest\", \"Xgboost\", \"SVM\", \"GradientBoosting\",\n",
        "            \"LightGBM\", \"CatBoost\", \"KNN\",\n",
        "            \"Linear\", \"Lasso\", \"Ridge\", \"ElasticNet\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Logistic Regression ---> max_iter = 2000 - 5000\n",
        "    if model_name == \"Logistic Regression\":\n",
        "        c = trial.suggest_float(\"lr_C\", 0.1, 100, log=True)\n",
        "        solver = trial.suggest_categorical(\"lr_solver\", [\"liblinear\", \"saga\", \"lbfgs\", \"newton-cg\"])\n",
        "        penalty = None\n",
        "        if solver in [\"lbfgs\", \"newton-cg\"]:\n",
        "            penalty = trial.suggest_categorical(\"lr_penalty_lbfgs_nc\", [\"l2\", None])\n",
        "        elif solver == \"liblinear\":\n",
        "            penalty = trial.suggest_categorical(\"lr_penalty_liblinear\", [\"l1\", \"l2\"])\n",
        "        else:  # saga\n",
        "            penalty = trial.suggest_categorical(\"lr_penalty_saga\", [\"l1\", \"l2\", \"elasticnet\", None])\n",
        "        l1_ratio = trial.suggest_float(\"lr_l1_ratio\", 0.0, 1.0) if penalty == \"elasticnet\" else None\n",
        "    #max_iter = trial.suggest_int(\"lr_max_iter\", 100, 5000, step=100)\n",
        "        model = LogisticRegression(\n",
        "            C=c, solver=solver, penalty=penalty,\n",
        "            l1_ratio=l1_ratio, max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    # Random Forest\n",
        "    elif model_name == \"Random Forest\":\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=trial.suggest_int(\"rf_n_estimators\", 50, 300),\n",
        "            max_depth=trial.suggest_int(\"rf_max_depth\", 3, 20),\n",
        "            min_samples_split=trial.suggest_int(\"rf_min_samples_split\", 2, 10),\n",
        "            min_samples_leaf=trial.suggest_int(\"rf_min_samples_leaf\", 1, 10),\n",
        "            bootstrap=trial.suggest_categorical(\"rf_bootstrap\", [True, False]),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    # XGBoost\n",
        "    elif model_name == \"Xgboost\":\n",
        "        model = XGBClassifier(\n",
        "            learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"xgb_max_depth\", 2, 12),\n",
        "            n_estimators=trial.suggest_int(\"xgb_n_estimators\", 50, 500),\n",
        "            subsample=trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
        "            colsample_bytree=trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0),\n",
        "            gamma=trial.suggest_float(\"xgb_gamma\", 0, 5),\n",
        "            min_child_weight=trial.suggest_int(\"xgb_min_child_weight\", 1, 10),\n",
        "            reg_alpha=trial.suggest_float(\"xgb_reg_alpha\", 0.0, 5.0),\n",
        "            reg_lambda=trial.suggest_float(\"xgb_reg_lambda\", 0.0, 5.0),\n",
        "            random_state=42, use_label_encoder=False, eval_metric=\"logloss\"\n",
        "        )\n",
        "\n",
        "    # SVM\n",
        "    elif model_name == \"SVM\":\n",
        "        model = SVC(\n",
        "            C=trial.suggest_float(\"svm_C\", 0.1, 100, log=True),\n",
        "            kernel=trial.suggest_categorical(\"svm_kernel\", [\"linear\", \"rbf\", \"poly\", \"sigmoid\"]),\n",
        "            gamma=trial.suggest_categorical(\"svm_gamma\", [\"scale\", \"auto\"]),\n",
        "            probability=True, random_state=42\n",
        "        )\n",
        "\n",
        "    # Gradient Boosting\n",
        "    elif model_name == \"GradientBoosting\":\n",
        "        model = GradientBoostingClassifier(\n",
        "            n_estimators=trial.suggest_int(\"gb_n_estimators\", 50, 500),\n",
        "            learning_rate=trial.suggest_float(\"gb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"gb_max_depth\", 2, 10),\n",
        "            subsample=trial.suggest_float(\"gb_subsample\", 0.5, 1.0),\n",
        "            min_samples_split=trial.suggest_int(\"gb_min_samples_split\", 2, 20),\n",
        "            min_samples_leaf=trial.suggest_int(\"gb_min_samples_leaf\", 1, 20),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    # LightGBM\n",
        "    elif model_name == \"LightGBM\":\n",
        "        model = LGBMClassifier(\n",
        "            n_estimators=trial.suggest_int(\"lgb_n_estimators\", 50, 500),\n",
        "            learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"lgb_max_depth\", -1, 20),\n",
        "            num_leaves=trial.suggest_int(\"lgb_num_leaves\", 20, 300),\n",
        "            subsample=trial.suggest_float(\"lgb_subsample\", 0.5, 1.0),\n",
        "            colsample_bytree=trial.suggest_float(\"lgb_colsample_bytree\", 0.5, 1.0),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    # CatBoost\n",
        "    elif model_name == \"CatBoost\":\n",
        "        model = CatBoostClassifier(\n",
        "            iterations=trial.suggest_int(\"cat_iterations\", 100, 500),\n",
        "            depth=trial.suggest_int(\"cat_depth\", 3, 10),\n",
        "            learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3, log=True),\n",
        "            l2_leaf_reg=trial.suggest_float(\"cat_l2_leaf_reg\", 1.0, 10.0),\n",
        "            verbose=0, random_state=42\n",
        "        )\n",
        "\n",
        "    # KNN\n",
        "    elif model_name == \"KNN\":\n",
        "        model = KNeighborsClassifier(\n",
        "            n_neighbors=trial.suggest_int(\"knn_n_neighbors\", 3, 30),\n",
        "            weights=trial.suggest_categorical(\"knn_weights\", [\"uniform\", \"distance\"]),\n",
        "            p=trial.suggest_int(\"knn_p\", 1, 2)\n",
        "        )\n",
        "\n",
        "    # Linear / Lasso / Ridge / ElasticNet\n",
        "    elif model_name == \"Linear\":\n",
        "        model = LogisticRegression(max_iter=5000, random_state=42)\n",
        "\n",
        "    elif model_name == \"Lasso\":\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"l1\", solver=\"saga\",\n",
        "            C=trial.suggest_float(\"lasso_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"Ridge\":\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"l2\", solver=\"saga\",\n",
        "            C=trial.suggest_float(\"ridge_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    else:  # ElasticNet\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"elasticnet\", solver=\"saga\",\n",
        "            l1_ratio=trial.suggest_float(\"elastic_l1_ratio\", 0.0, 1.0),\n",
        "            C=trial.suggest_float(\"elastic_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "   # Linear Regression (no penalty â†’ only certain solvers allowed)\n",
        "    elif model_name == \"Linear\":\n",
        "        solver = trial.suggest_categorical(\"linear_solver\", [\"lbfgs\", \"newton-cg\", \"sag\", \"saga\"])\n",
        "        model = LogisticRegression(\n",
        "            penalty=None, solver=solver,\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    # Lasso (L1)\n",
        "    elif model_name == \"Lasso\":\n",
        "        solver = trial.suggest_categorical(\"lasso_solver\", [\"liblinear\", \"saga\"])\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"l1\", solver=solver,\n",
        "            C=trial.suggest_float(\"lasso_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    # Ridge (L2)\n",
        "    elif model_name == \"Ridge\":\n",
        "        solver = trial.suggest_categorical(\"ridge_solver\", [\"lbfgs\", \"newton-cg\", \"sag\", \"saga\", \"liblinear\"])\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"l2\", solver=solver,\n",
        "            C=trial.suggest_float(\"ridge_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    # ElasticNet (only saga)\n",
        "    elif model_name == \"ElasticNet\":\n",
        "        model = LogisticRegression(\n",
        "            penalty=\"elasticnet\", solver=\"saga\",\n",
        "            l1_ratio=trial.suggest_float(\"elastic_l1_ratio\", 0.0, 1.0),\n",
        "            C=trial.suggest_float(\"elastic_C\", 0.01, 10, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "\n",
        "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optuna Objective Function - Regression**"
      ],
      "metadata": {
        "id": "AqxIv0DSMxKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_regression(trial):\n",
        "    model_name = trial.suggest_categorical(\n",
        "        \"model\",\n",
        "        [\n",
        "            \"Random Forest\", \"Xgboost\", \"SVM\", \"GradientBoosting\",\n",
        "            \"LightGBM\", \"CatBoost\", \"KNN\",\n",
        "            \"Linear\", \"Lasso\", \"Ridge\", \"ElasticNet\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if model_name == \"Random Forest\":\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=trial.suggest_int(\"rf_n_estimators\", 50, 300),\n",
        "            max_depth=trial.suggest_int(\"rf_max_depth\", 3, 20),\n",
        "            min_samples_split=trial.suggest_int(\"rf_min_samples_split\", 2, 10),\n",
        "            min_samples_leaf=trial.suggest_int(\"rf_min_samples_leaf\", 1, 10),\n",
        "            bootstrap=trial.suggest_categorical(\"rf_bootstrap\", [True, False]),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"Xgboost\":\n",
        "        model = XGBRegressor(\n",
        "            learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"xgb_max_depth\", 2, 12),\n",
        "            n_estimators=trial.suggest_int(\"xgb_n_estimators\", 50, 500),\n",
        "            subsample=trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
        "            colsample_bytree=trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0),\n",
        "            gamma=trial.suggest_float(\"xgb_gamma\", 0, 5),\n",
        "            reg_alpha=trial.suggest_float(\"xgb_reg_alpha\", 0.0, 5.0),\n",
        "            reg_lambda=trial.suggest_float(\"xgb_reg_lambda\", 0.0, 5.0),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"SVM\":\n",
        "        model = SVR(\n",
        "            C=trial.suggest_float(\"svm_C\", 0.1, 100, log=True),\n",
        "            kernel=trial.suggest_categorical(\"svm_kernel\", [\"linear\", \"rbf\", \"poly\", \"sigmoid\"]),\n",
        "            gamma=trial.suggest_categorical(\"svm_gamma\", [\"scale\", \"auto\"])\n",
        "        )\n",
        "\n",
        "    elif model_name == \"GradientBoosting\":\n",
        "        model = GradientBoostingRegressor(\n",
        "            n_estimators=trial.suggest_int(\"gb_n_estimators\", 50, 500),\n",
        "            learning_rate=trial.suggest_float(\"gb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"gb_max_depth\", 2, 10),\n",
        "            subsample=trial.suggest_float(\"gb_subsample\", 0.5, 1.0),\n",
        "            min_samples_split=trial.suggest_int(\"gb_min_samples_split\", 2, 20),\n",
        "            min_samples_leaf=trial.suggest_int(\"gb_min_samples_leaf\", 1, 20),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"LightGBM\":\n",
        "        model = LGBMRegressor(\n",
        "            n_estimators=trial.suggest_int(\"lgb_n_estimators\", 50, 500),\n",
        "            learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3, log=True),\n",
        "            max_depth=trial.suggest_int(\"lgb_max_depth\", -1, 20),\n",
        "            num_leaves=trial.suggest_int(\"lgb_num_leaves\", 20, 300),\n",
        "            subsample=trial.suggest_float(\"lgb_subsample\", 0.5, 1.0),\n",
        "            colsample_bytree=trial.suggest_float(\"lgb_colsample_bytree\", 0.5, 1.0),\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"CatBoost\":\n",
        "        model = CatBoostRegressor(\n",
        "            iterations=trial.suggest_int(\"cat_iterations\", 100, 500),\n",
        "            depth=trial.suggest_int(\"cat_depth\", 3, 10),\n",
        "            learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3, log=True),\n",
        "            l2_leaf_reg=trial.suggest_float(\"cat_l2_leaf_reg\", 1.0, 10.0),\n",
        "            verbose=0, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"KNN\":\n",
        "        model = KNeighborsRegressor(\n",
        "            n_neighbors=trial.suggest_int(\"knn_n_neighbors\", 3, 30),\n",
        "            weights=trial.suggest_categorical(\"knn_weights\", [\"uniform\", \"distance\"]),\n",
        "            p=trial.suggest_int(\"knn_p\", 1, 2)\n",
        "        )\n",
        "\n",
        "    elif model_name == \"Linear\":\n",
        "        model = LinearRegression()\n",
        "\n",
        "    elif model_name == \"Lasso\":\n",
        "        model = Lasso(\n",
        "            alpha=trial.suggest_float(\"lasso_alpha\", 0.0001, 1.0, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    elif model_name == \"Ridge\":\n",
        "        model = Ridge(\n",
        "            alpha=trial.suggest_float(\"ridge_alpha\", 0.0001, 10.0, log=True),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    else:  # ElasticNet\n",
        "        model = ElasticNet(\n",
        "            alpha=trial.suggest_float(\"elastic_alpha\", 0.0001, 1.0, log=True),\n",
        "            l1_ratio=trial.suggest_float(\"elastic_l1_ratio\", 0.0, 1.0),\n",
        "            max_iter=5000, random_state=42\n",
        "        )\n",
        "\n",
        "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"r2\").mean()\n",
        "    return score"
      ],
      "metadata": {
        "id": "Tabj2cdRMmLt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}