{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Random Forest\n",
    "# ==========================\n",
    "def objective_rf(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "    }\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# XGBoost\n",
    "# ==========================\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "    }\n",
    "    model = xgb.XGBClassifier(\n",
    "        **params, random_state=42, use_label_encoder=False, eval_metric=\"logloss\"\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# LightGBM\n",
    "# ==========================\n",
    "def objective_lgb(trial):\n",
    "    params = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 20),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 10),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 10)\n",
    "    }\n",
    "    model = lgb.LGBMClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# CatBoost\n",
    "# ==========================\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255)\n",
    "    }\n",
    "    model = CatBoostClassifier(\n",
    "        **params, random_state=42, verbose=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# KNN\n",
    "# ==========================\n",
    "def objective_knn(trial):\n",
    "    params = {\n",
    "        \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 1, 50),\n",
    "        \"weights\": trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "        \"p\": trial.suggest_int(\"p\", 1, 2)\n",
    "    }\n",
    "    model = KNeighborsClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Logistic Regression\n",
    "# ==========================\n",
    "def objective_logreg(trial):\n",
    "    params = {\n",
    "        \"penalty\": trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\", \"elasticnet\", \"none\"]),\n",
    "        \"C\": trial.suggest_float(\"C\", 0.001, 10, log=True),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"liblinear\", \"saga\", \"lbfgs\"]),\n",
    "    }\n",
    "    # elasticnet requires l1_ratio\n",
    "    if params[\"penalty\"] == \"elasticnet\":\n",
    "        params[\"l1_ratio\"] = trial.suggest_float(\"l1_ratio\", 0, 1)\n",
    "    model = LogisticRegression(max_iter=2000, random_state=42, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Decision Tree\n",
    "# ==========================\n",
    "def objective_dt(trial):\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "    }\n",
    "    model = DecisionTreeClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_rf, n_trials=50)\n",
    "print(\"Best RF params:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Random Forest Regressor\n",
    "# ==========================\n",
    "def objective_rf_reg(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "    }\n",
    "    model = RandomForestRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# XGBoost Regressor\n",
    "# ==========================\n",
    "def objective_xgb_reg(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# LightGBM Regressor\n",
    "# ==========================\n",
    "def objective_lgb_reg(trial):\n",
    "    params = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 20),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 10),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 10)\n",
    "    }\n",
    "    model = lgb.LGBMRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# CatBoost Regressor\n",
    "# ==========================\n",
    "def objective_cat_reg(trial):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255)\n",
    "    }\n",
    "    model = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# KNN Regressor\n",
    "# ==========================\n",
    "def objective_knn_reg(trial):\n",
    "    params = {\n",
    "        \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 1, 50),\n",
    "        \"weights\": trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "        \"p\": trial.suggest_int(\"p\", 1, 2)\n",
    "    }\n",
    "    model = KNeighborsRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "# ==========================\n",
    "# Decision Tree Regressor\n",
    "# ==========================\n",
    "def objective_dt_reg(trial):\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "    }\n",
    "    model = DecisionTreeRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Linear Regression\n",
    "# ==========================\n",
    "def objective_lin_reg(trial):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Ridge Regression\n",
    "# ==========================\n",
    "def objective_ridge(trial):\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.0001, 10, log=True)\n",
    "    }\n",
    "    model = Ridge(**params, random_state=42, max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Lasso Regression\n",
    "# ==========================\n",
    "def objective_lasso(trial):\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.0001, 10, log=True)\n",
    "    }\n",
    "    model = Lasso(**params, random_state=42, max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ElasticNet Regression\n",
    "# ==========================\n",
    "def objective_enet(trial):\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.0001, 10, log=True),\n",
    "        \"l1_ratio\": trial.suggest_float(\"l1_ratio\", 0, 1)\n",
    "    }\n",
    "    model = ElasticNet(**params, random_state=42, max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Polynomial Regression\n",
    "# ==========================\n",
    "def objective_poly(trial):\n",
    "    degree = trial.suggest_int(\"degree\", 2, 5)\n",
    "    model = Pipeline([\n",
    "        (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "        (\"lin_reg\", LinearRegression())\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_rf_reg, n_trials=50)\n",
    "print(\"Best RF Regressor params:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Classification Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_classification(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"DecisionTree\", \"XGBoost\", \n",
    "                                                     \"LightGBM\", \"CatBoost\", \"KNN\", \"LogisticRegression\"])\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Random Forest\n",
    "    # -----------------------------\n",
    "    if model_name == \"RandomForest\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"rf_n_estimators\", 100, 1000),\n",
    "            \"max_depth\": trial.suggest_int(\"rf_max_depth\", 3, 30),\n",
    "            \"min_samples_split\": trial.suggest_int(\"rf_min_samples_split\", 2, 20),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"rf_min_samples_leaf\", 1, 10),\n",
    "            \"max_features\": trial.suggest_categorical(\"rf_max_features\", [\"sqrt\", \"log2\", None]),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"rf_bootstrap\", [True, False])\n",
    "        }\n",
    "        model = RandomForestClassifier(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Decision Tree\n",
    "    # -----------------------------\n",
    "    elif model_name == \"DecisionTree\":\n",
    "        params = {\n",
    "            \"max_depth\": trial.suggest_int(\"dt_max_depth\", 3, 30),\n",
    "            \"min_samples_split\": trial.suggest_int(\"dt_min_samples_split\", 2, 20),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"dt_min_samples_leaf\", 1, 10),\n",
    "            \"max_features\": trial.suggest_categorical(\"dt_max_features\", [\"sqrt\", \"log2\", None])\n",
    "        }\n",
    "        model = DecisionTreeClassifier(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # XGBoost\n",
    "    # -----------------------------\n",
    "    elif model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 100, 1000),\n",
    "            \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 15),\n",
    "            \"subsample\": trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"xgb_gamma\", 0, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"xgb_min_child_weight\", 1, 10)\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**params, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # LightGBM\n",
    "    # -----------------------------\n",
    "    elif model_name == \"LightGBM\":\n",
    "        params = {\n",
    "            \"num_leaves\": trial.suggest_int(\"lgb_num_leaves\", 20, 200),\n",
    "            \"max_depth\": trial.suggest_int(\"lgb_max_depth\", -1, 20),\n",
    "            \"learning_rate\": trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3),\n",
    "            \"n_estimators\": trial.suggest_int(\"lgb_n_estimators\", 100, 1000),\n",
    "            \"min_child_samples\": trial.suggest_int(\"lgb_min_child_samples\", 5, 50),\n",
    "            \"subsample\": trial.suggest_float(\"lgb_subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"lgb_colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"lgb_reg_alpha\", 0, 10),\n",
    "            \"reg_lambda\": trial.suggest_float(\"lgb_reg_lambda\", 0, 10)\n",
    "        }\n",
    "        model = lgb.LGBMClassifier(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # CatBoost\n",
    "    # -----------------------------\n",
    "    elif model_name == \"CatBoost\":\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"cat_iterations\", 100, 1000),\n",
    "            \"depth\": trial.suggest_int(\"cat_depth\", 3, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"cat_l2_leaf_reg\", 1, 10),\n",
    "            \"border_count\": trial.suggest_int(\"cat_border_count\", 32, 255)\n",
    "        }\n",
    "        model = CatBoostClassifier(**params, random_state=42, verbose=0)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # KNN\n",
    "    # -----------------------------\n",
    "    elif model_name == \"KNN\":\n",
    "        params = {\n",
    "            \"n_neighbors\": trial.suggest_int(\"knn_n_neighbors\", 1, 50),\n",
    "            \"weights\": trial.suggest_categorical(\"knn_weights\", [\"uniform\", \"distance\"]),\n",
    "            \"p\": trial.suggest_int(\"knn_p\", 1, 2)\n",
    "        }\n",
    "        model = KNeighborsClassifier(**params)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Logistic Regression\n",
    "    # -----------------------------\n",
    "    elif model_name == \"LogisticRegression\":\n",
    "        params = {\n",
    "            \"penalty\": trial.suggest_categorical(\"log_penalty\", [\"l1\", \"l2\", \"elasticnet\", \"none\"]),\n",
    "            \"C\": trial.suggest_float(\"log_C\", 0.001, 10, log=True),\n",
    "            \"solver\": trial.suggest_categorical(\"log_solver\", [\"liblinear\", \"saga\", \"lbfgs\"])\n",
    "        }\n",
    "        if params[\"penalty\"] == \"elasticnet\":\n",
    "            params[\"l1_ratio\"] = trial.suggest_float(\"log_l1_ratio\", 0, 1)\n",
    "        model = LogisticRegression(max_iter=2000, random_state=42, **params)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Fit & Evaluate\n",
    "    # -----------------------------\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Regression Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_regression(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"DecisionTree\", \"XGBoost\", \n",
    "                                                     \"LightGBM\", \"CatBoost\", \"KNN\", \n",
    "                                                     \"Linear\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"Polynomial\"])\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Random Forest\n",
    "    # -----------------------------\n",
    "    if model_name == \"RandomForest\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"rf_n_estimators\", 100, 1000),\n",
    "            \"max_depth\": trial.suggest_int(\"rf_max_depth\", 3, 30),\n",
    "            \"min_samples_split\": trial.suggest_int(\"rf_min_samples_split\", 2, 20),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"rf_min_samples_leaf\", 1, 10),\n",
    "            \"max_features\": trial.suggest_categorical(\"rf_max_features\", [\"sqrt\", \"log2\", None]),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"rf_bootstrap\", [True, False])\n",
    "        }\n",
    "        model = RandomForestRegressor(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Decision Tree\n",
    "    # -----------------------------\n",
    "    elif model_name == \"DecisionTree\":\n",
    "        params = {\n",
    "            \"max_depth\": trial.suggest_int(\"dt_max_depth\", 3, 30),\n",
    "            \"min_samples_split\": trial.suggest_int(\"dt_min_samples_split\", 2, 20),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"dt_min_samples_leaf\", 1, 10),\n",
    "            \"max_features\": trial.suggest_categorical(\"dt_max_features\", [\"sqrt\", \"log2\", None])\n",
    "        }\n",
    "        model = DecisionTreeRegressor(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # XGBoost\n",
    "    # -----------------------------\n",
    "    elif model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 100, 1000),\n",
    "            \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 15),\n",
    "            \"subsample\": trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"xgb_gamma\", 0, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"xgb_min_child_weight\", 1, 10)\n",
    "        }\n",
    "        model = xgb.XGBRegressor(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # LightGBM\n",
    "    # -----------------------------\n",
    "    elif model_name == \"LightGBM\":\n",
    "        params = {\n",
    "            \"num_leaves\": trial.suggest_int(\"lgb_num_leaves\", 20, 200),\n",
    "            \"max_depth\": trial.suggest_int(\"lgb_max_depth\", -1, 20),\n",
    "            \"learning_rate\": trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3),\n",
    "            \"n_estimators\": trial.suggest_int(\"lgb_n_estimators\", 100, 1000),\n",
    "            \"min_child_samples\": trial.suggest_int(\"lgb_min_child_samples\", 5, 50),\n",
    "            \"subsample\": trial.suggest_float(\"lgb_subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"lgb_colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"lgb_reg_alpha\", 0, 10),\n",
    "            \"reg_lambda\": trial.suggest_float(\"lgb_reg_lambda\", 0, 10)\n",
    "        }\n",
    "        model = lgb.LGBMRegressor(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # CatBoost\n",
    "    # -----------------------------\n",
    "    elif model_name == \"CatBoost\":\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"cat_iterations\", 100, 1000),\n",
    "            \"depth\": trial.suggest_int(\"cat_depth\", 3, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"cat_l2_leaf_reg\", 1, 10),\n",
    "            \"border_count\": trial.suggest_int(\"cat_border_count\", 32, 255)\n",
    "        }\n",
    "        model = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # KNN\n",
    "    # -----------------------------\n",
    "    elif model_name == \"KNN\":\n",
    "        params = {\n",
    "            \"n_neighbors\": trial.suggest_int(\"knn_n_neighbors\", 1, 50),\n",
    "            \"weights\": trial.suggest_categorical(\"knn_weights\", [\"uniform\", \"distance\"]),\n",
    "            \"p\": trial.suggest_int(\"knn_p\", 1, 2)\n",
    "        }\n",
    "        model = KNeighborsRegressor(**params)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Linear Regression\n",
    "    # -----------------------------\n",
    "    elif model_name == \"Linear\":\n",
    "        model = LinearRegression()\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Ridge\n",
    "    # -----------------------------\n",
    "    elif model_name == \"Ridge\":\n",
    "        alpha = trial.suggest_float(\"ridge_alpha\", 0.0001, 10, log=True)\n",
    "        model = Ridge(alpha=alpha, random_state=42, max_iter=2000)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Lasso\n",
    "    # -----------------------------\n",
    "    elif model_name == \"Lasso\":\n",
    "        alpha = trial.suggest_float(\"lasso_alpha\", 0.0001, 10, log=True)\n",
    "        model = Lasso(alpha=alpha, random_state=42, max_iter=2000)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # ElasticNet\n",
    "    # -----------------------------\n",
    "    elif model_name == \"ElasticNet\":\n",
    "        alpha = trial.suggest_float(\"enet_alpha\", 0.0001, 10, log=True)\n",
    "        l1_ratio = trial.suggest_float(\"enet_l1_ratio\", 0, 1)\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=2000)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Polynomial Regression\n",
    "    # -----------------------------\n",
    "    elif model_name == \"Polynomial\":\n",
    "        degree = trial.suggest_int(\"poly_degree\", 2, 5)\n",
    "        model = Pipeline([\n",
    "            (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "            (\"lin_reg\", LinearRegression())\n",
    "        ])\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Fit & Evaluate\n",
    "    # -----------------------------\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is for liitle less processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Classification Objective (Essential Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_classification(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"DecisionTree\", \"XGBoost\", \n",
    "                                                     \"LightGBM\", \"CatBoost\", \"KNN\", \"LogisticRegression\"])\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=trial.suggest_int(\"rf_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"rf_max_depth\", 5, 15),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"DecisionTree\":\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=trial.suggest_int(\"dt_max_depth\", 5, 15),\n",
    "            min_samples_leaf=trial.suggest_int(\"dt_min_samples_leaf\", 1, 5),\n",
    "            criterion=trial.suggest_categorical(\"dt_criterion\", [\"gini\", \"entropy\"]),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"XGBoost\":\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=trial.suggest_int(\"xgb_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"xgb_max_depth\", 3, 7),\n",
    "            learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.05, 0.2),\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\"\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"LightGBM\":\n",
    "        model = lgb.LGBMClassifier(\n",
    "            n_estimators=trial.suggest_int(\"lgb_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"lgb_max_depth\", 3, 10),\n",
    "            num_leaves=trial.suggest_int(\"lgb_num_leaves\", 20, 50),\n",
    "            learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.05, 0.15),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"CatBoost\":\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=trial.suggest_int(\"cat_iterations\", 100, 200),\n",
    "            depth=trial.suggest_int(\"cat_depth\", 3, 6),\n",
    "            learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.05, 0.2),\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"KNN\":\n",
    "        model = KNeighborsClassifier(\n",
    "            n_neighbors=trial.suggest_int(\"knn_n_neighbors\", 3, 15)\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"LogisticRegression\":\n",
    "        model = LogisticRegression(\n",
    "            C=trial.suggest_float(\"log_C\", 0.1, 1.0),\n",
    "            penalty=\"l2\",\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=2000,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_name == \"SVM\":\n",
    "        model = SVC(\n",
    "            C=trial.suggest_float(\"svm_C\", 0.1, 10.0, log=True),\n",
    "            kernel=trial.suggest_categorical(\"svm_kernel\", [\"linear\", \"rbf\"]),\n",
    "            gamma=trial.suggest_categorical(\"svm_gamma\", [\"scale\", \"auto\"]),\n",
    "            probability=True,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_name == \"SVM\":\n",
    "        C = trial.suggest_float(\"svm_C\", 0.1, 100.0, log=True)\n",
    "        kernel = trial.suggest_categorical(\"svm_kernel\", [\"linear\", \"rbf\", \"poly\", \"sigmoid\"])\n",
    "        gamma = trial.suggest_categorical(\"svm_gamma\", [\"scale\", \"auto\"])\n",
    "        degree = trial.suggest_int(\"svm_degree\", 2, 5) if kernel == \"poly\" else 3\n",
    "\n",
    "        model = SVC(\n",
    "            C=C,\n",
    "            kernel=kernel,\n",
    "            gamma=gamma,\n",
    "            degree=degree,\n",
    "            probability=True,   # needed for predict_proba()\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Regression Objective (Essential Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_regression(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"DecisionTree\", \"XGBoost\", \n",
    "                                                     \"LightGBM\", \"CatBoost\", \"KNN\", \n",
    "                                                     \"Linear\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"Polynomial\"])\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=trial.suggest_int(\"rf_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"rf_max_depth\", 5, 15),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"DecisionTree\":\n",
    "        model = DecisionTreeRegressor(\n",
    "            max_depth=trial.suggest_int(\"dt_max_depth\", 5, 15),\n",
    "            min_samples_leaf=trial.suggest_int(\"dt_min_samples_leaf\", 1, 5),\n",
    "            criterion=trial.suggest_categorical(\"dt_criterion\", [\"squared_error\", \"friedman_mse\", \"absolute_error\"]),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    \n",
    "    elif model_name == \"XGBoost\":\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=trial.suggest_int(\"xgb_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"xgb_max_depth\", 3, 7),\n",
    "            learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.05, 0.2),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"LightGBM\":\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=trial.suggest_int(\"lgb_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"lgb_max_depth\", 3, 10),\n",
    "            num_leaves=trial.suggest_int(\"lgb_num_leaves\", 20, 50),\n",
    "            learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.05, 0.15),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"CatBoost\":\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=trial.suggest_int(\"cat_iterations\", 100, 200),\n",
    "            depth=trial.suggest_int(\"cat_depth\", 3, 6),\n",
    "            learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.05, 0.2),\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"KNN\":\n",
    "        model = KNeighborsRegressor(\n",
    "            n_neighbors=trial.suggest_int(\"knn_n_neighbors\", 3, 15)\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"Linear\":\n",
    "        model = LinearRegression()\n",
    "    \n",
    "    elif model_name == \"Ridge\":\n",
    "        model = Ridge(\n",
    "            alpha=trial.suggest_float(\"ridge_alpha\", 0.1, 1.0),\n",
    "            random_state=42,\n",
    "            max_iter=2000\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"Lasso\":\n",
    "        model = Lasso(\n",
    "            alpha=trial.suggest_float(\"lasso_alpha\", 0.1, 1.0),\n",
    "            random_state=42,\n",
    "            max_iter=2000\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"ElasticNet\":\n",
    "        model = ElasticNet(\n",
    "            alpha=trial.suggest_float(\"enet_alpha\", 0.1, 1.0),\n",
    "            l1_ratio=trial.suggest_float(\"enet_l1_ratio\", 0, 1),\n",
    "            random_state=42,\n",
    "            max_iter=2000\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"Polynomial\":\n",
    "        degree = trial.suggest_int(\"poly_degree\", 2, 3)\n",
    "        model = Pipeline([\n",
    "            (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "            (\"lin_reg\", LinearRegression())\n",
    "        ])\n",
    "\n",
    "    elif model_name == \"SVR\":\n",
    "        model = SVR(\n",
    "            C=trial.suggest_float(\"svr_C\", 0.1, 10.0, log=True),\n",
    "            kernel=trial.suggest_categorical(\"svr_kernel\", [\"linear\", \"rbf\"]),\n",
    "            gamma=trial.suggest_categorical(\"svr_gamma\", [\"scale\", \"auto\"])\n",
    "        )\n",
    "\n",
    "    elif model_name == \"SVR\":\n",
    "        C = trial.suggest_float(\"svr_C\", 0.1, 100.0, log=True)\n",
    "        kernel = trial.suggest_categorical(\"svr_kernel\", [\"linear\", \"rbf\", \"poly\", \"sigmoid\"])\n",
    "        gamma = trial.suggest_categorical(\"svr_gamma\", [\"scale\", \"auto\"])\n",
    "        degree = trial.suggest_int(\"svr_degree\", 2, 5) if kernel == \"poly\" else 3\n",
    "\n",
    "        model = SVR(\n",
    "            C=C,\n",
    "            kernel=kernel,\n",
    "            gamma=gamma,\n",
    "            degree=degree\n",
    "        )\n",
    "\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deepseek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective_classification(trial, X, y):\n",
    "    # Let the trial suggest which model to use\n",
    "    classifier_name = trial.suggest_categorical('classifier', [\n",
    "        'RandomForest', 'XGBoost', 'LightGBM', 'CatBoost', \n",
    "        'KNN', 'LogisticRegression', 'DecisionTree', 'SVM'\n",
    "    ])\n",
    "    \n",
    "    if classifier_name == 'RandomForest':\n",
    "        n_estimators = trial.suggest_int('rf_n_estimators', 50, 500)\n",
    "        max_depth = trial.suggest_int('rf_max_depth', 3, 20)\n",
    "        min_samples_split = trial.suggest_int('rf_min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "        max_features = trial.suggest_categorical('rf_max_features', ['sqrt', 'log2', None])\n",
    "        \n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "    elif classifier_name == 'XGBoost':\n",
    "        n_estimators = trial.suggest_int('xgb_n_estimators', 50, 500)\n",
    "        max_depth = trial.suggest_int('xgb_max_depth', 3, 15)\n",
    "        learning_rate = trial.suggest_float('xgb_learning_rate', 0.01, 0.3, log=True)\n",
    "        subsample = trial.suggest_float('xgb_subsample', 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float('xgb_colsample_bytree', 0.6, 1.0)\n",
    "        reg_alpha = trial.suggest_float('xgb_reg_alpha', 0, 10)\n",
    "        reg_lambda = trial.suggest_float('xgb_reg_lambda', 1, 10)\n",
    "        \n",
    "        model = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        \n",
    "    elif classifier_name == 'LightGBM':\n",
    "        n_estimators = trial.suggest_int('lgb_n_estimators', 50, 500)\n",
    "        num_leaves = trial.suggest_int('lgb_num_leaves', 20, 150)\n",
    "        learning_rate = trial.suggest_float('lgb_learning_rate', 0.01, 0.3, log=True)\n",
    "        subsample = trial.suggest_float('lgb_subsample', 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float('lgb_colsample_bytree', 0.6, 1.0)\n",
    "        reg_alpha = trial.suggest_float('lgb_reg_alpha', 0, 10)\n",
    "        reg_lambda = trial.suggest_float('lgb_reg_lambda', 0, 10)\n",
    "        \n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            num_leaves=num_leaves,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "    elif classifier_name == 'CatBoost':\n",
    "        iterations = trial.suggest_int('cb_iterations', 50, 500)\n",
    "        depth = trial.suggest_int('cb_depth', 4, 10)\n",
    "        learning_rate = trial.suggest_float('cb_learning_rate', 0.01, 0.3, log=True)\n",
    "        l2_leaf_reg = trial.suggest_float('cb_l2_leaf_reg', 1, 10)\n",
    "        border_count = trial.suggest_int('cb_border_count', 32, 255)\n",
    "        \n",
    "        model = CatBoostClassifier(\n",
    "            iterations=iterations,\n",
    "            depth=depth,\n",
    "            learning_rate=learning_rate,\n",
    "            l2_leaf_reg=l2_leaf_reg,\n",
    "            border_count=border_count,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "    elif classifier_name == 'KNN':\n",
    "        n_neighbors = trial.suggest_int('knn_n_neighbors', 3, 50)\n",
    "        weights = trial.suggest_categorical('knn_weights', ['uniform', 'distance'])\n",
    "        p = trial.suggest_int('knn_p', 1, 2)  # 1: Manhattan, 2: Euclidean\n",
    "        \n",
    "        model = KNeighborsClassifier(\n",
    "            n_neighbors=n_neighbors,\n",
    "            weights=weights,\n",
    "            p=p,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "    elif classifier_name == 'LogisticRegression':\n",
    "        C = trial.suggest_float('lr_C', 0.01, 10, log=True)\n",
    "        penalty = trial.suggest_categorical('lr_penalty', ['l2', 'none'])\n",
    "        solver = 'lbfgs' if penalty == 'l2' else 'saga'\n",
    "        \n",
    "        model = LogisticRegression(\n",
    "            C=C,\n",
    "            penalty=penalty,\n",
    "            solver=solver,\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "    elif classifier_name == 'DecisionTree':\n",
    "        max_depth = trial.suggest_int('dt_max_depth', 3, 20)\n",
    "        min_samples_split = trial.suggest_int('dt_min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('dt_min_samples_leaf', 1, 10)\n",
    "        max_features = trial.suggest_categorical('dt_max_features', ['sqrt', 'log2', None])\n",
    "        \n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "    elif classifier_name == 'SVM':\n",
    "        C = trial.suggest_float('svm_C', 0.1, 10, log=True)\n",
    "        kernel = trial.suggest_categorical('svm_kernel', ['linear', 'rbf', 'poly'])\n",
    "        gamma = trial.suggest_categorical('svm_gamma', ['scale', 'auto'])\n",
    "        \n",
    "        model = SVC(\n",
    "            C=C,\n",
    "            kernel=kernel,\n",
    "            gamma=gamma,\n",
    "            random_state=42,\n",
    "            probability=True\n",
    "        )\n",
    "    \n",
    "    # Use cross-validation for more robust evaluation\n",
    "    score = cross_val_score(model, X, y, n_jobs=-1, cv=5, scoring='f1_macro').mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def objective_regression(trial, X, y):\n",
    "    # Let the trial suggest which model to use\n",
    "    regressor_name = trial.suggest_categorical('regressor', [\n",
    "        'RandomForest', 'XGBoost', 'LightGBM', 'CatBoost', \n",
    "        'KNN', 'LinearRegression', 'Ridge', 'Lasso', \n",
    "        'ElasticNet', 'Polynomial', 'DecisionTree', 'SVM'\n",
    "    ])\n",
    "    \n",
    "    if regressor_name == 'RandomForest':\n",
    "        n_estimators = trial.suggest_int('rf_n_estimators', 50, 500)\n",
    "        max_depth = trial.suggest_int('rf_max_depth', 3, 20)\n",
    "        min_samples_split = trial.suggest_int('rf_min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "        max_features = trial.suggest_categorical('rf_max_features', ['sqrt', 'log2', None])\n",
    "        \n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "    elif regressor_name == 'XGBoost':\n",
    "        n_estimators = trial.suggest_int('xgb_n_estimators', 50, 500)\n",
    "        max_depth = trial.suggest_int('xgb_max_depth', 3, 15)\n",
    "        learning_rate = trial.suggest_float('xgb_learning_rate', 0.01, 0.3, log=True)\n",
    "        subsample = trial.suggest_float('xgb_subsample', 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float('xgb_colsample_bytree', 0.6, 1.0)\n",
    "        reg_alpha = trial.suggest_float('xgb_reg_alpha', 0, 10)\n",
    "        reg_lambda = trial.suggest_float('xgb_reg_lambda', 1, 10)\n",
    "        \n",
    "        model = XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "    elif regressor_name == 'LightGBM':\n",
    "        n_estimators = trial.suggest_int('lgb_n_estimators', 50, 500)\n",
    "        num_leaves = trial.suggest_int('lgb_num_leaves', 20, 150)\n",
    "        learning_rate = trial.suggest_float('lgb_learning_rate', 0.01, 0.3, log=True)\n",
    "        subsample = trial.suggest_float('lgb_subsample', 0.6, 1.0)\n",
    "        colsample_bytree = trial.suggest_float('lgb_colsample_bytree', 0.6, 1.0)\n",
    "        reg_alpha = trial.suggest_float('lgb_reg_alpha', 0, 10)\n",
    "        reg_lambda = trial.suggest_float('lgb_reg_lambda', 0, 10)\n",
    "        \n",
    "        model = LGBMRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            num_leaves=num_leaves,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "    elif regressor_name == 'CatBoost':\n",
    "        iterations = trial.suggest_int('cb_iterations', 50, 500)\n",
    "        depth = trial.suggest_int('cb_depth', 4, 10)\n",
    "        learning_rate = trial.suggest_float('cb_learning_rate', 0.01, 0.3, log=True)\n",
    "        l2_leaf_reg = trial.suggest_float('cb_l2_leaf_reg', 1, 10)\n",
    "        border_count = trial.suggest_int('cb_border_count', 32, 255)\n",
    "        \n",
    "        model = CatBoostRegressor(\n",
    "            iterations=iterations,\n",
    "            depth=depth,\n",
    "            learning_rate=learning_rate,\n",
    "            l2_leaf_reg=l2_leaf_reg,\n",
    "            border_count=border_count,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "    elif regressor_name == 'KNN':\n",
    "        n_neighbors = trial.suggest_int('knn_n_neighbors', 3, 50)\n",
    "        weights = trial.suggest_categorical('knn_weights', ['uniform', 'distance'])\n",
    "        p = trial.suggest_int('knn_p', 1, 2)\n",
    "        \n",
    "        model = KNeighborsRegressor(\n",
    "            n_neighbors=n_neighbors,\n",
    "            weights=weights,\n",
    "            p=p,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "    elif regressor_name == 'LinearRegression':\n",
    "        # Linear regression has no hyperparameters to tune\n",
    "        model = LinearRegression(n_jobs=-1)\n",
    "        \n",
    "    elif regressor_name == 'Ridge':\n",
    "        alpha = trial.suggest_float('ridge_alpha', 0.1, 10, log=True)\n",
    "        \n",
    "        model = Ridge(alpha=alpha, random_state=42)\n",
    "        \n",
    "    elif regressor_name == 'Lasso':\n",
    "        alpha = trial.suggest_float('lasso_alpha', 0.1, 10, log=True)\n",
    "        \n",
    "        model = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n",
    "        \n",
    "    elif regressor_name == 'ElasticNet':\n",
    "        alpha = trial.suggest_float('enet_alpha', 0.1, 10, log=True)\n",
    "        l1_ratio = trial.suggest_float('enet_l1_ratio', 0, 1)\n",
    "        \n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=10000)\n",
    "        \n",
    "    elif regressor_name == 'Polynomial':\n",
    "        degree = trial.suggest_int('poly_degree', 2, 4)\n",
    "        include_bias = trial.suggest_categorical('poly_include_bias', [True, False])\n",
    "        \n",
    "        # Create pipeline with polynomial features and linear regression\n",
    "        model = Pipeline([\n",
    "            ('poly', PolynomialFeatures(degree=degree, include_bias=include_bias)),\n",
    "            ('linear', LinearRegression())\n",
    "        ])\n",
    "        \n",
    "    elif regressor_name == 'DecisionTree':\n",
    "        max_depth = trial.suggest_int('dt_max_depth', 3, 20)\n",
    "        min_samples_split = trial.suggest_int('dt_min_samples_split', 2, 20)\n",
    "        min_samples_leaf = trial.suggest_int('dt_min_samples_leaf', 1, 10)\n",
    "        max_features = trial.suggest_categorical('dt_max_features', ['sqrt', 'log2', None])\n",
    "        \n",
    "        model = DecisionTreeRegressor(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "    elif regressor_name == 'SVM':\n",
    "        C = trial.suggest_float('svm_C', 0.1, 10, log=True)\n",
    "        kernel = trial.suggest_categorical('svm_kernel', ['linear', 'rbf', 'poly'])\n",
    "        gamma = trial.suggest_categorical('svm_gamma', ['scale', 'auto'])\n",
    "        \n",
    "        model = SVR(C=C, kernel=kernel, gamma=gamma)\n",
    "    \n",
    "    # Use cross-validation for more robust evaluation\n",
    "    score = cross_val_score(model, X, y, n_jobs=-1, cv=5, scoring='r2').mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: objective_classification(trial, X_train, y_train), n_trials=100)\n",
    "\n",
    "# For regression\n",
    "study = optuna.create_study(direction='maximize')  # maximize R score\n",
    "study.optimize(lambda trial: objective_regression(trial, X_train, y_train), n_trials=100)\n",
    "\n",
    "# Get best parameters\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best score:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Objective Function Templates\n",
    "# Classification Objective (if/elif block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_classification(trial):\n",
    "    model_type = trial.suggest_categorical('model_type', [\n",
    "        'random_forest', 'xgboost', 'lightgbm', 'catboost',\n",
    "        'knn', 'logistic_regression', 'decision_tree', 'svm'\n",
    "    ])\n",
    "\n",
    "    if model_type == 'random_forest':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 40),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "            'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "        }\n",
    "        model = RandomForestClassifier(**params)\n",
    "    elif model_type == 'xgboost':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True)\n",
    "        }\n",
    "        model = XGBClassifier(**params)\n",
    "    elif model_type == 'lightgbm':\n",
    "        params = {\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100)\n",
    "        }\n",
    "        model = LGBMClassifier(**params)\n",
    "    elif model_type == 'catboost':\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "            'depth': trial.suggest_int('depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "            'border_count': trial.suggest_int('border_count', 32, 128)\n",
    "        }\n",
    "        model = CatBoostClassifier(**params)\n",
    "    elif model_type == 'knn':\n",
    "        params = {\n",
    "            'n_neighbors': trial.suggest_int('n_neighbors', 3, 30),\n",
    "            'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "            'p': trial.suggest_int('p', 1, 2)\n",
    "        }\n",
    "        model = KNeighborsClassifier(**params)\n",
    "    elif model_type == 'logistic_regression':\n",
    "        params = {\n",
    "            'C': trial.suggest_float('C', 0.001, 50),\n",
    "            'penalty': trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet', 'none']),\n",
    "            'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n",
    "            'max_iter': trial.suggest_int('max_iter', 100, 1000)\n",
    "        }\n",
    "        model = LogisticRegression(**params)\n",
    "    elif model_type == 'decision_tree':\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 40),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "        }\n",
    "        model = DecisionTreeClassifier(**params)\n",
    "    elif model_type == 'svm':\n",
    "        params = {\n",
    "            'C': trial.suggest_float('C', 0.01, 100),\n",
    "            'kernel': trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid']),\n",
    "            'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "        }\n",
    "        model = SVC(**params)\n",
    "\n",
    "    score = cross_val_score(model, X, y, cv=3, scoring='accuracy').mean()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Objective (adds linear, ridge, lasso, elasticnet, polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_regression(trial):\n",
    "    model_type = trial.suggest_categorical('model_type', [\n",
    "        'random_forest', 'xgboost', 'lightgbm', 'catboost', 'knn',\n",
    "        'linear', 'ridge', 'lasso', 'elasticnet', 'polynomial',\n",
    "        'decision_tree', 'svm'\n",
    "    ])\n",
    "\n",
    "    if model_type == 'random_forest':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 40),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "            'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "        }\n",
    "        model = RandomForestRegressor(**params)\n",
    "    elif model_type == 'xgboost':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True)\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "    elif model_type == 'lightgbm':\n",
    "        params = {\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100)\n",
    "        }\n",
    "        model = LGBMRegressor(**params)\n",
    "    elif model_type == 'catboost':\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "            'depth': trial.suggest_int('depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "            'border_count': trial.suggest_int('border_count', 32, 128)\n",
    "        }\n",
    "        model = CatBoostRegressor(**params)\n",
    "    elif model_type == 'knn':\n",
    "        params = {\n",
    "            'n_neighbors': trial.suggest_int('n_neighbors', 3, 30),\n",
    "            'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "            'p': trial.suggest_int('p', 1, 2)\n",
    "        }\n",
    "        model = KNeighborsRegressor(**params)\n",
    "    elif model_type == 'linear':\n",
    "        model = LinearRegression()\n",
    "    elif model_type == 'ridge':\n",
    "        params = {\n",
    "            'alpha': trial.suggest_float('alpha', 0.01, 100),\n",
    "            'solver': trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'])\n",
    "        }\n",
    "        model = Ridge(**params)\n",
    "    elif model_type == 'lasso':\n",
    "        params = {\n",
    "            'alpha': trial.suggest_float('alpha', 0.01, 1.0),\n",
    "            'max_iter': trial.suggest_int('max_iter', 1000, 5000)\n",
    "        }\n",
    "        model = Lasso(**params)\n",
    "    elif model_type == 'elasticnet':\n",
    "        params = {\n",
    "            'alpha': trial.suggest_float('alpha', 0.01, 1.0),\n",
    "            'l1_ratio': trial.suggest_float('l1_ratio', 0.1, 1.0),\n",
    "            'max_iter': trial.suggest_int('max_iter', 1000, 5000)\n",
    "        }\n",
    "        model = ElasticNet(**params)\n",
    "    elif model_type == 'polynomial':\n",
    "        degree = trial.suggest_int('degree', 2, 5)\n",
    "        model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    elif model_type == 'decision_tree':\n",
    "        params = {\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 40),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'criterion': trial.suggest_categorical('criterion', ['mse', 'friedman_mse'])\n",
    "        }\n",
    "        model = DecisionTreeRegressor(**params)\n",
    "    elif model_type == 'svm':\n",
    "        params = {\n",
    "            'C': trial.suggest_float('C', 0.01, 100),\n",
    "            'kernel': trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid']),\n",
    "            'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "        }\n",
    "        model = SVR(**params)\n",
    "\n",
    "    score = cross_val_score(model, X, y, cv=3, scoring='neg_mean_squared_error').mean()\n",
    "    return score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
