{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Random Forest\n",
    "# ==========================\n",
    "def objective_rf(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "    }\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# XGBoost\n",
    "# ==========================\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "    }\n",
    "    model = xgb.XGBClassifier(\n",
    "        **params, random_state=42, use_label_encoder=False, eval_metric=\"logloss\"\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# LightGBM\n",
    "# ==========================\n",
    "def objective_lgb(trial):\n",
    "    params = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 20),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 10),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 10)\n",
    "    }\n",
    "    model = lgb.LGBMClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# CatBoost\n",
    "# ==========================\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255)\n",
    "    }\n",
    "    model = CatBoostClassifier(\n",
    "        **params, random_state=42, verbose=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# KNN\n",
    "# ==========================\n",
    "def objective_knn(trial):\n",
    "    params = {\n",
    "        \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 1, 50),\n",
    "        \"weights\": trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "        \"p\": trial.suggest_int(\"p\", 1, 2)\n",
    "    }\n",
    "    model = KNeighborsClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Logistic Regression\n",
    "# ==========================\n",
    "def objective_logreg(trial):\n",
    "    params = {\n",
    "        \"penalty\": trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\", \"elasticnet\", \"none\"]),\n",
    "        \"C\": trial.suggest_float(\"C\", 0.001, 10, log=True),\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"liblinear\", \"saga\", \"lbfgs\"]),\n",
    "    }\n",
    "    # elasticnet requires l1_ratio\n",
    "    if params[\"penalty\"] == \"elasticnet\":\n",
    "        params[\"l1_ratio\"] = trial.suggest_float(\"l1_ratio\", 0, 1)\n",
    "    model = LogisticRegression(max_iter=2000, random_state=42, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Decision Tree\n",
    "# ==========================\n",
    "def objective_dt(trial):\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "    }\n",
    "    model = DecisionTreeClassifier(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_rf, n_trials=50)\n",
    "print(\"Best RF params:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Random Forest Regressor\n",
    "# ==========================\n",
    "def objective_rf_reg(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "    }\n",
    "    model = RandomForestRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# XGBoost Regressor\n",
    "# ==========================\n",
    "def objective_xgb_reg(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10)\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# LightGBM Regressor\n",
    "# ==========================\n",
    "def objective_lgb_reg(trial):\n",
    "    params = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 20),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 10),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 10)\n",
    "    }\n",
    "    model = lgb.LGBMRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# CatBoost Regressor\n",
    "# ==========================\n",
    "def objective_cat_reg(trial):\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255)\n",
    "    }\n",
    "    model = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# KNN Regressor\n",
    "# ==========================\n",
    "def objective_knn_reg(trial):\n",
    "    params = {\n",
    "        \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 1, 50),\n",
    "        \"weights\": trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "        \"p\": trial.suggest_int(\"p\", 1, 2)\n",
    "    }\n",
    "    model = KNeighborsRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "# ==========================\n",
    "# Decision Tree Regressor\n",
    "# ==========================\n",
    "def objective_dt_reg(trial):\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "    }\n",
    "    model = DecisionTreeRegressor(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Linear Regression\n",
    "# ==========================\n",
    "def objective_lin_reg(trial):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Ridge Regression\n",
    "# ==========================\n",
    "def objective_ridge(trial):\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.0001, 10, log=True)\n",
    "    }\n",
    "    model = Ridge(**params, random_state=42, max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Lasso Regression\n",
    "# ==========================\n",
    "def objective_lasso(trial):\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.0001, 10, log=True)\n",
    "    }\n",
    "    model = Lasso(**params, random_state=42, max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ElasticNet Regression\n",
    "# ==========================\n",
    "def objective_enet(trial):\n",
    "    params = {\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.0001, 10, log=True),\n",
    "        \"l1_ratio\": trial.suggest_float(\"l1_ratio\", 0, 1)\n",
    "    }\n",
    "    model = ElasticNet(**params, random_state=42, max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Polynomial Regression\n",
    "# ==========================\n",
    "def objective_poly(trial):\n",
    "    degree = trial.suggest_int(\"degree\", 2, 5)\n",
    "    model = Pipeline([\n",
    "        (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "        (\"lin_reg\", LinearRegression())\n",
    "    ])\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_rf_reg, n_trials=50)\n",
    "print(\"Best RF Regressor params:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Classification Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_classification(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"DecisionTree\", \"XGBoost\", \n",
    "                                                     \"LightGBM\", \"CatBoost\", \"KNN\", \"LogisticRegression\"])\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Random Forest\n",
    "    # -----------------------------\n",
    "    if model_name == \"RandomForest\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"rf_n_estimators\", 100, 1000),\n",
    "            \"max_depth\": trial.suggest_int(\"rf_max_depth\", 3, 30),\n",
    "            \"min_samples_split\": trial.suggest_int(\"rf_min_samples_split\", 2, 20),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"rf_min_samples_leaf\", 1, 10),\n",
    "            \"max_features\": trial.suggest_categorical(\"rf_max_features\", [\"sqrt\", \"log2\", None]),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"rf_bootstrap\", [True, False])\n",
    "        }\n",
    "        model = RandomForestClassifier(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Decision Tree\n",
    "    # -----------------------------\n",
    "    elif model_name == \"DecisionTree\":\n",
    "        params = {\n",
    "            \"max_depth\": trial.suggest_int(\"dt_max_depth\", 3, 30),\n",
    "            \"min_samples_split\": trial.suggest_int(\"dt_min_samples_split\", 2, 20),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"dt_min_samples_leaf\", 1, 10),\n",
    "            \"max_features\": trial.suggest_categorical(\"dt_max_features\", [\"sqrt\", \"log2\", None])\n",
    "        }\n",
    "        model = DecisionTreeClassifier(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # XGBoost\n",
    "    # -----------------------------\n",
    "    elif model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 100, 1000),\n",
    "            \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 15),\n",
    "            \"subsample\": trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"xgb_gamma\", 0, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"xgb_min_child_weight\", 1, 10)\n",
    "        }\n",
    "        model = xgb.XGBClassifier(**params, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # LightGBM\n",
    "    # -----------------------------\n",
    "    elif model_name == \"LightGBM\":\n",
    "        params = {\n",
    "            \"num_leaves\": trial.suggest_int(\"lgb_num_leaves\", 20, 200),\n",
    "            \"max_depth\": trial.suggest_int(\"lgb_max_depth\", -1, 20),\n",
    "            \"learning_rate\": trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3),\n",
    "            \"n_estimators\": trial.suggest_int(\"lgb_n_estimators\", 100, 1000),\n",
    "            \"min_child_samples\": trial.suggest_int(\"lgb_min_child_samples\", 5, 50),\n",
    "            \"subsample\": trial.suggest_float(\"lgb_subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"lgb_colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"lgb_reg_alpha\", 0, 10),\n",
    "            \"reg_lambda\": trial.suggest_float(\"lgb_reg_lambda\", 0, 10)\n",
    "        }\n",
    "        model = lgb.LGBMClassifier(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # CatBoost\n",
    "    # -----------------------------\n",
    "    elif model_name == \"CatBoost\":\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"cat_iterations\", 100, 1000),\n",
    "            \"depth\": trial.suggest_int(\"cat_depth\", 3, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"cat_l2_leaf_reg\", 1, 10),\n",
    "            \"border_count\": trial.suggest_int(\"cat_border_count\", 32, 255)\n",
    "        }\n",
    "        model = CatBoostClassifier(**params, random_state=42, verbose=0)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # KNN\n",
    "    # -----------------------------\n",
    "    elif model_name == \"KNN\":\n",
    "        params = {\n",
    "            \"n_neighbors\": trial.suggest_int(\"knn_n_neighbors\", 1, 50),\n",
    "            \"weights\": trial.suggest_categorical(\"knn_weights\", [\"uniform\", \"distance\"]),\n",
    "            \"p\": trial.suggest_int(\"knn_p\", 1, 2)\n",
    "        }\n",
    "        model = KNeighborsClassifier(**params)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Logistic Regression\n",
    "    # -----------------------------\n",
    "    elif model_name == \"LogisticRegression\":\n",
    "        params = {\n",
    "            \"penalty\": trial.suggest_categorical(\"log_penalty\", [\"l1\", \"l2\", \"elasticnet\", \"none\"]),\n",
    "            \"C\": trial.suggest_float(\"log_C\", 0.001, 10, log=True),\n",
    "            \"solver\": trial.suggest_categorical(\"log_solver\", [\"liblinear\", \"saga\", \"lbfgs\"])\n",
    "        }\n",
    "        if params[\"penalty\"] == \"elasticnet\":\n",
    "            params[\"l1_ratio\"] = trial.suggest_float(\"log_l1_ratio\", 0, 1)\n",
    "        model = LogisticRegression(max_iter=2000, random_state=42, **params)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Fit & Evaluate\n",
    "    # -----------------------------\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Regression Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_regression(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"DecisionTree\", \"XGBoost\", \n",
    "                                                     \"LightGBM\", \"CatBoost\", \"KNN\", \n",
    "                                                     \"Linear\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"Polynomial\"])\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Random Forest\n",
    "    # -----------------------------\n",
    "    if model_name == \"RandomForest\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"rf_n_estimators\", 100, 1000),\n",
    "            \"max_depth\": trial.suggest_int(\"rf_max_depth\", 3, 30),\n",
    "            \"min_samples_split\": trial.suggest_int(\"rf_min_samples_split\", 2, 20),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"rf_min_samples_leaf\", 1, 10),\n",
    "            \"max_features\": trial.suggest_categorical(\"rf_max_features\", [\"sqrt\", \"log2\", None]),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"rf_bootstrap\", [True, False])\n",
    "        }\n",
    "        model = RandomForestRegressor(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Decision Tree\n",
    "    # -----------------------------\n",
    "    elif model_name == \"DecisionTree\":\n",
    "        params = {\n",
    "            \"max_depth\": trial.suggest_int(\"dt_max_depth\", 3, 30),\n",
    "            \"min_samples_split\": trial.suggest_int(\"dt_min_samples_split\", 2, 20),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"dt_min_samples_leaf\", 1, 10),\n",
    "            \"max_features\": trial.suggest_categorical(\"dt_max_features\", [\"sqrt\", \"log2\", None])\n",
    "        }\n",
    "        model = DecisionTreeRegressor(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # XGBoost\n",
    "    # -----------------------------\n",
    "    elif model_name == \"XGBoost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"xgb_n_estimators\", 100, 1000),\n",
    "            \"learning_rate\": trial.suggest_float(\"xgb_learning_rate\", 0.01, 0.3),\n",
    "            \"max_depth\": trial.suggest_int(\"xgb_max_depth\", 3, 15),\n",
    "            \"subsample\": trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"xgb_gamma\", 0, 10),\n",
    "            \"min_child_weight\": trial.suggest_int(\"xgb_min_child_weight\", 1, 10)\n",
    "        }\n",
    "        model = xgb.XGBRegressor(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # LightGBM\n",
    "    # -----------------------------\n",
    "    elif model_name == \"LightGBM\":\n",
    "        params = {\n",
    "            \"num_leaves\": trial.suggest_int(\"lgb_num_leaves\", 20, 200),\n",
    "            \"max_depth\": trial.suggest_int(\"lgb_max_depth\", -1, 20),\n",
    "            \"learning_rate\": trial.suggest_float(\"lgb_learning_rate\", 0.01, 0.3),\n",
    "            \"n_estimators\": trial.suggest_int(\"lgb_n_estimators\", 100, 1000),\n",
    "            \"min_child_samples\": trial.suggest_int(\"lgb_min_child_samples\", 5, 50),\n",
    "            \"subsample\": trial.suggest_float(\"lgb_subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"lgb_colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"lgb_reg_alpha\", 0, 10),\n",
    "            \"reg_lambda\": trial.suggest_float(\"lgb_reg_lambda\", 0, 10)\n",
    "        }\n",
    "        model = lgb.LGBMRegressor(**params, random_state=42)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # CatBoost\n",
    "    # -----------------------------\n",
    "    elif model_name == \"CatBoost\":\n",
    "        params = {\n",
    "            \"iterations\": trial.suggest_int(\"cat_iterations\", 100, 1000),\n",
    "            \"depth\": trial.suggest_int(\"cat_depth\", 3, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"cat_learning_rate\", 0.01, 0.3),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"cat_l2_leaf_reg\", 1, 10),\n",
    "            \"border_count\": trial.suggest_int(\"cat_border_count\", 32, 255)\n",
    "        }\n",
    "        model = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # KNN\n",
    "    # -----------------------------\n",
    "    elif model_name == \"KNN\":\n",
    "        params = {\n",
    "            \"n_neighbors\": trial.suggest_int(\"knn_n_neighbors\", 1, 50),\n",
    "            \"weights\": trial.suggest_categorical(\"knn_weights\", [\"uniform\", \"distance\"]),\n",
    "            \"p\": trial.suggest_int(\"knn_p\", 1, 2)\n",
    "        }\n",
    "        model = KNeighborsRegressor(**params)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Linear Regression\n",
    "    # -----------------------------\n",
    "    elif model_name == \"Linear\":\n",
    "        model = LinearRegression()\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Ridge\n",
    "    # -----------------------------\n",
    "    elif model_name == \"Ridge\":\n",
    "        alpha = trial.suggest_float(\"ridge_alpha\", 0.0001, 10, log=True)\n",
    "        model = Ridge(alpha=alpha, random_state=42, max_iter=2000)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Lasso\n",
    "    # -----------------------------\n",
    "    elif model_name == \"Lasso\":\n",
    "        alpha = trial.suggest_float(\"lasso_alpha\", 0.0001, 10, log=True)\n",
    "        model = Lasso(alpha=alpha, random_state=42, max_iter=2000)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # ElasticNet\n",
    "    # -----------------------------\n",
    "    elif model_name == \"ElasticNet\":\n",
    "        alpha = trial.suggest_float(\"enet_alpha\", 0.0001, 10, log=True)\n",
    "        l1_ratio = trial.suggest_float(\"enet_l1_ratio\", 0, 1)\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=2000)\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Polynomial Regression\n",
    "    # -----------------------------\n",
    "    elif model_name == \"Polynomial\":\n",
    "        degree = trial.suggest_int(\"poly_degree\", 2, 5)\n",
    "        model = Pipeline([\n",
    "            (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "            (\"lin_reg\", LinearRegression())\n",
    "        ])\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Fit & Evaluate\n",
    "    # -----------------------------\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Classification Objective (Essential Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_classification(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"DecisionTree\", \"XGBoost\", \n",
    "                                                     \"LightGBM\", \"CatBoost\", \"KNN\", \"LogisticRegression\"])\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=trial.suggest_int(\"rf_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"rf_max_depth\", 5, 15),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"DecisionTree\":\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=trial.suggest_int(\"dt_max_depth\", 5, 15),\n",
    "            min_samples_leaf=trial.suggest_int(\"dt_min_samples_leaf\", 1, 5),\n",
    "            criterion=trial.suggest_categorical(\"dt_criterion\", [\"gini\", \"entropy\"]),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"XGBoost\":\n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=trial.suggest_int(\"xgb_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"xgb_max_depth\", 3, 7),\n",
    "            learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.05, 0.2),\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric=\"logloss\"\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"LightGBM\":\n",
    "        model = lgb.LGBMClassifier(\n",
    "            n_estimators=trial.suggest_int(\"lgb_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"lgb_max_depth\", 3, 10),\n",
    "            num_leaves=trial.suggest_int(\"lgb_num_leaves\", 20, 50),\n",
    "            learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.05, 0.15),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"CatBoost\":\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=trial.suggest_int(\"cat_iterations\", 100, 200),\n",
    "            depth=trial.suggest_int(\"cat_depth\", 3, 6),\n",
    "            learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.05, 0.2),\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"KNN\":\n",
    "        model = KNeighborsClassifier(\n",
    "            n_neighbors=trial.suggest_int(\"knn_n_neighbors\", 3, 15)\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"LogisticRegression\":\n",
    "        model = LogisticRegression(\n",
    "            C=trial.suggest_float(\"log_C\", 0.1, 1.0),\n",
    "            penalty=\"l2\",\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=2000,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Regression Objective (Essential Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_regression(trial):\n",
    "    model_name = trial.suggest_categorical(\"model\", [\"RandomForest\", \"DecisionTree\", \"XGBoost\", \n",
    "                                                     \"LightGBM\", \"CatBoost\", \"KNN\", \n",
    "                                                     \"Linear\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"Polynomial\"])\n",
    "    \n",
    "    if model_name == \"RandomForest\":\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=trial.suggest_int(\"rf_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"rf_max_depth\", 5, 15),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"DecisionTree\":\n",
    "        model = DecisionTreeRegressor(\n",
    "            max_depth=trial.suggest_int(\"dt_max_depth\", 5, 15),\n",
    "            min_samples_leaf=trial.suggest_int(\"dt_min_samples_leaf\", 1, 5),\n",
    "            criterion=trial.suggest_categorical(\"dt_criterion\", [\"squared_error\", \"friedman_mse\", \"absolute_error\"]),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    \n",
    "    elif model_name == \"XGBoost\":\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=trial.suggest_int(\"xgb_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"xgb_max_depth\", 3, 7),\n",
    "            learning_rate=trial.suggest_float(\"xgb_learning_rate\", 0.05, 0.2),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"LightGBM\":\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=trial.suggest_int(\"lgb_n_estimators\", 100, 200),\n",
    "            max_depth=trial.suggest_int(\"lgb_max_depth\", 3, 10),\n",
    "            num_leaves=trial.suggest_int(\"lgb_num_leaves\", 20, 50),\n",
    "            learning_rate=trial.suggest_float(\"lgb_learning_rate\", 0.05, 0.15),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"CatBoost\":\n",
    "        model = CatBoostRegressor(\n",
    "            iterations=trial.suggest_int(\"cat_iterations\", 100, 200),\n",
    "            depth=trial.suggest_int(\"cat_depth\", 3, 6),\n",
    "            learning_rate=trial.suggest_float(\"cat_learning_rate\", 0.05, 0.2),\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"KNN\":\n",
    "        model = KNeighborsRegressor(\n",
    "            n_neighbors=trial.suggest_int(\"knn_n_neighbors\", 3, 15)\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"Linear\":\n",
    "        model = LinearRegression()\n",
    "    \n",
    "    elif model_name == \"Ridge\":\n",
    "        model = Ridge(\n",
    "            alpha=trial.suggest_float(\"ridge_alpha\", 0.1, 1.0),\n",
    "            random_state=42,\n",
    "            max_iter=2000\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"Lasso\":\n",
    "        model = Lasso(\n",
    "            alpha=trial.suggest_float(\"lasso_alpha\", 0.1, 1.0),\n",
    "            random_state=42,\n",
    "            max_iter=2000\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"ElasticNet\":\n",
    "        model = ElasticNet(\n",
    "            alpha=trial.suggest_float(\"enet_alpha\", 0.1, 1.0),\n",
    "            l1_ratio=trial.suggest_float(\"enet_l1_ratio\", 0, 1),\n",
    "            random_state=42,\n",
    "            max_iter=2000\n",
    "        )\n",
    "    \n",
    "    elif model_name == \"Polynomial\":\n",
    "        degree = trial.suggest_int(\"poly_degree\", 2, 3)\n",
    "        model = Pipeline([\n",
    "            (\"poly\", PolynomialFeatures(degree=degree)),\n",
    "            (\"lin_reg\", LinearRegression())\n",
    "        ])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_squared_error(y_valid, preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
